from pyspark.sql import SparkSession
from pyspark.sql.functions import col, round as spark_round, when, concat, lit
import os

def create_spark_session():
    spark = SparkSession.builder \
        .appName("å¯¼å‡ºä¸­æ–‡åˆ—åCSVæ•°æ®") \
        .enableHiveSupport() \
        .getOrCreate()
    
    spark.sql("USE dbestate")
    return spark

def check_table_structure(spark):
    """æ£€æŸ¥è¡¨ç»“æ„"""
    print("=== æ£€æŸ¥ç°æœ‰è¡¨ç»“æ„ ===")
    
    tables = ["city_job_summary", "job_category_summary", "job_ml_features", "house_ml_features", "ml_predictions"]
    
    for table in tables:
        try:
            print(f"\n{table} è¡¨ç»“æ„:")
            desc = spark.sql(f"DESCRIBE dbestate.{table}")
            desc.show()
        except Exception as e:
            print(f"âŒ {table} è¡¨ä¸å­˜åœ¨: {e}")

def export_city_job_summary(spark):
    """å¯¼å‡ºåŸå¸‚å·¥ä½œæ±‡æ€» - ä¸­æ–‡åˆ—å"""
    print("=== å¯¼å‡ºåŸå¸‚å·¥ä½œæ±‡æ€»æ•°æ® ===")
    
    city_job_df = spark.sql("""
        SELECT 
            city,
            job_count,
            ROUND(avg_salary, 0) as avg_salary
        FROM city_job_summary
        ORDER BY avg_salary DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    city_job_result = city_job_df.select(
        col("city").alias("åŸå¸‚"),
        col("job_count").alias("å·¥ä½œæ•°é‡"),
        col("avg_salary").alias("å¹³å‡è–ªèµ„_å…ƒ")
    )
    
    city_job_result.show()
    return city_job_result

def export_job_category_summary(spark):
    """å¯¼å‡ºèŒä¸šç±»å‹æ±‡æ€» - ä¸­æ–‡åˆ—åï¼ˆä¿®å¤ç‰ˆï¼‰"""
    print("=== å¯¼å‡ºèŒä¸šç±»å‹æ±‡æ€»æ•°æ® ===")
    
    # ç›´æ¥ä»åŸå§‹è¡¨æŸ¥è¯¢ï¼Œè®¡ç®—ä½å®¿æ¯”ä¾‹
    job_category_df = spark.sql("""
        SELECT 
            job_category,
            COUNT(*) as job_count,
            ROUND(AVG(salary_avg), 0) as avg_salary,
            ROUND(SUM(has_accommodation) * 100.0 / COUNT(*), 1) as accommodation_ratio
        FROM job_ml_features
        GROUP BY job_category
        ORDER BY avg_salary DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    job_category_result = job_category_df.select(
        col("job_category").alias("èŒä¸šç±»å‹"),
        col("job_count").alias("å²—ä½æ•°é‡"),
        col("avg_salary").alias("å¹³å‡è–ªèµ„_å…ƒ"),
        col("accommodation_ratio").alias("åŒ…ä½å®¿æ¯”ä¾‹_ç™¾åˆ†æ¯”")
    )
    
    job_category_result.show()
    return job_category_result

def export_job_welfare_analysis(spark):
    """å¯¼å‡ºå·¥ä½œç¦åˆ©åˆ†æ - ä¸­æ–‡åˆ—åï¼ˆä¿®å¤ç‰ˆï¼‰"""
    print("=== å¯¼å‡ºå·¥ä½œç¦åˆ©åˆ†ææ•°æ® ===")
    
    # ç›´æ¥ä»job_ml_featuresè®¡ç®—ç¦åˆ©åˆ†æ
    welfare_df = spark.sql("""
        SELECT 
            city,
            job_category,
            COUNT(*) as total_jobs,
            SUM(has_accommodation) as accommodation_jobs,
            SUM(has_insurance) as insurance_jobs,
            ROUND(SUM(has_accommodation) * 100.0 / COUNT(*), 1) as accommodation_rate,
            ROUND(SUM(has_insurance) * 100.0 / COUNT(*), 1) as insurance_rate,
            ROUND(AVG(salary_avg), 0) as avg_salary
        FROM job_ml_features
        GROUP BY city, job_category
        ORDER BY city, avg_salary DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    welfare_result = welfare_df.select(
        col("city").alias("åŸå¸‚"),
        col("job_category").alias("èŒä¸šç±»å‹"),
        col("total_jobs").alias("æ€»å²—ä½æ•°"),
        col("accommodation_jobs").alias("åŒ…ä½å®¿å²—ä½æ•°"),
        col("insurance_jobs").alias("æœ‰ä¿é™©å²—ä½æ•°"),
        col("accommodation_rate").alias("ä½å®¿æä¾›ç‡_ç™¾åˆ†æ¯”"),
        col("insurance_rate").alias("ä¿é™©æä¾›ç‡_ç™¾åˆ†æ¯”"),
        col("avg_salary").alias("å¹³å‡è–ªèµ„_å…ƒ")
    )
    
    welfare_result.show()
    return welfare_result

def export_investment_analysis(spark):
    """å¯¼å‡ºæŠ•èµ„åˆ†æ - ä¸­æ–‡åˆ—åï¼ˆä¿®å¤ç‰ˆï¼‰"""
    print("=== å¯¼å‡ºæŠ•èµ„åˆ†ææ•°æ® ===")
    
    # ç›´æ¥è®¡ç®—æŠ•èµ„åˆ†æï¼Œä¸ä¾èµ–é¢„å­˜è¡¨
    investment_df = spark.sql("""
        SELECT 
            j.city,
            ROUND(j.avg_salary, 0) as monthly_salary,
            ROUND(h.avg_price, 0) as price_per_sqm,
            ROUND(h.avg_area, 1) as avg_area,
            ROUND(h.avg_price * h.avg_area, 0) as total_house_price,
            ROUND((h.avg_price * h.avg_area) / (j.avg_salary * 12), 1) as years_to_buy,
            CASE 
                WHEN (h.avg_price * h.avg_area) / (j.avg_salary * 12) <= 15 THEN 'å®¹æ˜“è´­ä¹°'
                WHEN (h.avg_price * h.avg_area) / (j.avg_salary * 12) <= 25 THEN 'ä¸­ç­‰éš¾åº¦'
                ELSE 'å›°éš¾è´­ä¹°'
            END as purchase_difficulty
        FROM city_job_summary j
        JOIN (
            SELECT 
                city,
                AVG(price_num) as avg_price,
                AVG(area_avg) as avg_area
            FROM house_ml_features
            GROUP BY city
        ) h ON j.city = h.city
        ORDER BY years_to_buy
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    investment_result = investment_df.select(
        col("city").alias("åŸå¸‚"),
        col("monthly_salary").alias("æœˆè–ªèµ„_å…ƒ"),
        col("price_per_sqm").alias("æˆ¿ä»·_å…ƒæ¯å¹³ç±³"),
        col("avg_area").alias("å¹³å‡é¢ç§¯_å¹³ç±³"),
        col("total_house_price").alias("æˆ¿å±‹æ€»ä»·_å…ƒ"),
        col("years_to_buy").alias("è´­æˆ¿æ‰€éœ€å¹´æ•°"),
        col("purchase_difficulty").alias("è´­æˆ¿éš¾åº¦")
    )
    
    investment_result.show()
    return investment_result

def export_job_detail_data(spark):
    """å¯¼å‡ºå·¥ä½œè¯¦ç»†æ•°æ® - ä¸­æ–‡åˆ—å"""
    print("=== å¯¼å‡ºå·¥ä½œè¯¦ç»†æ•°æ® ===")
    
    job_detail_df = spark.sql("""
        SELECT 
            city,
            job_category,
            job_name,
            address,
            ROUND(salary_avg, 0) as salary_avg,
            salary_label,
            has_accommodation,
            has_insurance,
            is_urgent,
            is_daily_pay,
            no_experience_required
        FROM job_ml_features
        ORDER BY city, salary_avg DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    job_detail_result = job_detail_df.select(
        col("city").alias("åŸå¸‚"),
        col("job_category").alias("èŒä¸šç±»å‹"),
        col("job_name").alias("èŒä½åç§°"),
        col("address").alias("å·¥ä½œåœ°å€"),
        col("salary_avg").alias("å¹³å‡è–ªèµ„_å…ƒ"),
        col("salary_label").alias("è–ªèµ„ç­‰çº§"),
        when(col("has_accommodation") == 1, "åŒ…ä½å®¿").otherwise("ä¸åŒ…ä½å®¿").alias("ä½å®¿æƒ…å†µ"),
        when(col("has_insurance") == 1, "æœ‰ä¿é™©").otherwise("æ— ä¿é™©").alias("ä¿é™©æƒ…å†µ"),
        when(col("is_urgent") == 1, "æ€¥æ‹›").otherwise("æ™®é€š").alias("æ‹›è˜ç´§æ€¥åº¦"),
        when(col("is_daily_pay") == 1, "æ—¥ç»“").otherwise("æœˆç»“").alias("è–ªèµ„ç»“ç®—æ–¹å¼"),
        when(col("no_experience_required") == 1, "æ— ç»éªŒè¦æ±‚").otherwise("æœ‰ç»éªŒè¦æ±‚").alias("ç»éªŒè¦æ±‚")
    )
    
    job_detail_result.show(10)
    return job_detail_result

def export_house_detail_data(spark):
    """å¯¼å‡ºæˆ¿ä»·è¯¦ç»†æ•°æ® - ä¸­æ–‡åˆ—å"""
    print("=== å¯¼å‡ºæˆ¿ä»·è¯¦ç»†æ•°æ® ===")
    
    house_detail_df = spark.sql("""
        SELECT 
            city,
            ROUND(area_avg, 1) as area_avg,
            room_count,
            ROUND(price_num, 0) as price_num,
            price_label,
            is_subway,
            has_parking
        FROM house_ml_features
        ORDER BY city, price_num DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    house_detail_result = house_detail_df.select(
        col("city").alias("åŸå¸‚"),
        col("area_avg").alias("é¢ç§¯_å¹³ç±³"),
        col("room_count").alias("æˆ¿é—´æ•°é‡"),
        col("price_num").alias("ä»·æ ¼_å…ƒæ¯å¹³ç±³"),
        col("price_label").alias("ä»·æ ¼ç­‰çº§"),
        when(col("is_subway") == 1, "åœ°é“æˆ¿").otherwise("éåœ°é“æˆ¿").alias("åœ°é“æƒ…å†µ"),
        when(col("has_parking") == 1, "æœ‰åœè½¦ä½").otherwise("æ— åœè½¦ä½").alias("åœè½¦æƒ…å†µ")
    )
    
    house_detail_result.show(10)
    return house_detail_result

def export_prediction_accuracy(spark):
    """å¯¼å‡ºé¢„æµ‹å‡†ç¡®ç‡æ•°æ® - ä¸­æ–‡åˆ—å"""
    print("=== å¯¼å‡ºé¢„æµ‹å‡†ç¡®ç‡æ•°æ® ===")
    
    # æˆ¿ä»·é¢„æµ‹å‡†ç¡®ç‡
    house_accuracy_df = spark.sql("""
        SELECT 
            city,
            COUNT(*) as total_predictions,
            SUM(CASE WHEN price_level = prediction THEN 1 ELSE 0 END) as correct_predictions,
            ROUND(SUM(CASE WHEN price_level = prediction THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as accuracy_rate
        FROM ml_predictions
        GROUP BY city
        ORDER BY accuracy_rate DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—å
    house_accuracy_result = house_accuracy_df.select(
        col("city").alias("åŸå¸‚"),
        col("total_predictions").alias("é¢„æµ‹æ€»æ•°"),
        col("correct_predictions").alias("é¢„æµ‹æ­£ç¡®æ•°é‡"),
        col("accuracy_rate").alias("é¢„æµ‹å‡†ç¡®ç‡_ç™¾åˆ†æ¯”")
    )
    
    house_accuracy_result.show()
    return house_accuracy_result

def export_comprehensive_city_summary(spark):
    """å¯¼å‡ºåŸå¸‚ç»¼åˆæ±‡æ€» - æŒ‰ç…§ä½ çš„æ ¼å¼"""
    print("=== å¯¼å‡ºåŸå¸‚ç»¼åˆæ±‡æ€»æ•°æ® ===")
    
    comprehensive_df = spark.sql("""
        SELECT 
            h.city,
            h.house_count,
            ROUND(h.avg_price, 1) as avg_price,
            ROUND(h.avg_area, 1) as avg_area,
            h.subway_count,
            ROUND(h.subway_ratio, 1) as subway_ratio,
            COALESCE(a.correct_predictions, 0) as correct_predictions,
            ROUND(COALESCE(a.accuracy_rate, 0), 1) as accuracy_rate
        FROM (
            SELECT 
                city,
                COUNT(*) as house_count,
                AVG(price_num) as avg_price,
                AVG(area_avg) as avg_area,
                SUM(is_subway) as subway_count,
                SUM(is_subway) * 100.0 / COUNT(*) as subway_ratio
            FROM house_ml_features
            GROUP BY city
        ) h
        LEFT JOIN (
            SELECT 
                city,
                SUM(CASE WHEN price_level = prediction THEN 1 ELSE 0 END) as correct_predictions,
                SUM(CASE WHEN price_level = prediction THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as accuracy_rate
            FROM ml_predictions
            GROUP BY city
        ) a ON h.city = a.city
        ORDER BY h.avg_price DESC
    """)
    
    # é‡å‘½åä¸ºä¸­æ–‡åˆ—åï¼Œæ ¼å¼ä¸ä½ çš„ç¤ºä¾‹ä¸€è‡´
    city_summary_result = comprehensive_df.select(
        col("city").alias("åŸå¸‚"),
        col("house_count").alias("æˆ¿æºæ•°é‡"),
        col("avg_price").alias("å¹³å‡ä»·æ ¼_å…ƒæ¯å¹³ç±³"),
        col("avg_area").alias("å¹³å‡é¢ç§¯_å¹³ç±³"),
        col("subway_count").alias("åœ°é“æˆ¿æ•°é‡"),
        col("subway_ratio").alias("åœ°é“æˆ¿æ¯”ä¾‹_ç™¾åˆ†æ¯”"),
        col("correct_predictions").alias("é¢„æµ‹æ­£ç¡®æ•°é‡"),
        col("accuracy_rate").alias("é¢„æµ‹å‡†ç¡®ç‡_ç™¾åˆ†æ¯”")
    )
    
    city_summary_result.show()
    return city_summary_result

def save_to_csv(df, filename, output_path):
    """ä¿å­˜DataFrameä¸ºCSVæ–‡ä»¶"""
    try:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        
        # è½¬æ¢ä¸ºPandas DataFrameå¹¶ä¿å­˜
        pandas_df = df.toPandas()
        full_path = os.path.join(output_path, filename)
        pandas_df.to_csv(full_path, index=False, encoding='utf-8-sig')
        
        print(f"{filename} å·²ä¿å­˜åˆ° {full_path}")
        return True
    except Exception as e:
        print(f"ä¿å­˜ {filename} å¤±è´¥: {e}")
        return False

def main():
    spark = create_spark_session()
    
    # è¾“å‡ºè·¯å¾„
    output_path = "/analysisProject/job_data"
    
    try:
        print("å¼€å§‹å¯¼å‡ºä¸­æ–‡åˆ—åCSVæ•°æ®")
        print("=" * 60)
        
        # 0. æ£€æŸ¥è¡¨ç»“æ„ï¼ˆå¯é€‰ï¼‰
        # check_table_structure(spark)
        
        # 1. åŸå¸‚å·¥ä½œæ±‡æ€»
        city_job_df = export_city_job_summary(spark)
        save_to_csv(city_job_df, "city_job_summary.csv", output_path)
        
        # 2. èŒä¸šç±»å‹æ±‡æ€»ï¼ˆä¿®å¤ç‰ˆï¼‰
        job_category_df = export_job_category_summary(spark)
        save_to_csv(job_category_df, "job_category_summary.csv", output_path)
        
        # 3. å·¥ä½œç¦åˆ©åˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰
        welfare_df = export_job_welfare_analysis(spark)
        save_to_csv(welfare_df, "job_welfare_analysis.csv", output_path)
        
        # 4. æŠ•èµ„åˆ†æï¼ˆä¿®å¤ç‰ˆï¼‰
        investment_df = export_investment_analysis(spark)
        save_to_csv(investment_df, "investment_analysis.csv", output_path)
        
        # 5. å·¥ä½œè¯¦ç»†æ•°æ®
        job_detail_df = export_job_detail_data(spark)
        save_to_csv(job_detail_df, "job_detail_data.csv", output_path)
        
        # 6. æˆ¿ä»·è¯¦ç»†æ•°æ®
        house_detail_df = export_house_detail_data(spark)
        save_to_csv(house_detail_df, "house_detail_data.csv", output_path)
        
        # 7. é¢„æµ‹å‡†ç¡®ç‡
        accuracy_df = export_prediction_accuracy(spark)
        save_to_csv(accuracy_df, "prediction_accuracy.csv", output_path)
        
        # 8. åŸå¸‚ç»¼åˆæ±‡æ€»ï¼ˆæŒ‰ä½ çš„æ ¼å¼ï¼‰
        city_summary_df = export_comprehensive_city_summary(spark)
        save_to_csv(city_summary_df, "city_summary.csv", output_path)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æ•°æ®å¯¼å‡ºå®Œæˆï¼")
        print(f"æ–‡ä»¶ä¿å­˜ä½ç½®: {output_path}")
        print("\nå¯¼å‡ºçš„æ–‡ä»¶åˆ—è¡¨:")
        print("   - city_job_summary.csv (åŸå¸‚å·¥ä½œæ±‡æ€»)")
        print("   - job_category_summary.csv (èŒä¸šç±»å‹æ±‡æ€»)")
        print("   - job_welfare_analysis.csv (å·¥ä½œç¦åˆ©åˆ†æ)")
        print("   - investment_analysis.csv (æŠ•èµ„åˆ†æ)")
        print("   - job_detail_data.csv (å·¥ä½œè¯¦ç»†æ•°æ®)")
        print("   - house_detail_data.csv (æˆ¿ä»·è¯¦ç»†æ•°æ®)")
        print("   - prediction_accuracy.csv (é¢„æµ‹å‡†ç¡®ç‡)")
        print("   - city_summary.csv (åŸå¸‚ç»¼åˆæ±‡æ€»)")
        print("\næ‰€æœ‰åˆ—åå‡ä¸ºä¸­æ–‡ï¼Œä¾¿äºFineBIç›´æ¥ä½¿ç”¨ï¼")
        
        # æ˜¾ç¤ºæ–‡ä»¶å¤§å°
        try:
            import subprocess
            result = subprocess.run(['ls', '-lh', output_path], capture_output=True, text=True)
            print(f"\n æ–‡ä»¶è¯¦æƒ…:")
            print(result.stdout)
        except:
            pass
        
    except Exception as e:
        print(f"å¯¼å‡ºè¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        spark.stop()

if __name__ == "__main__":
    main()

